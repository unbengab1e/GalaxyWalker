{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import MllamaConfig, MllamaForConditionalGeneration\n",
    "from transformers.models.mllama.configuration_mllama import MllamaTextConfig,MllamaVisionConfig\n",
    "from transformers.models.mllama.modeling_mllama import MllamaCrossAttentionDecoderLayer\n",
    "from transformers.utils import logging\n",
    "from transformers.modeling_rope_utils import rope_config_validation\n",
    "from typing import Dict, List, Optional, Union\n",
    "from models import AstrollavaConfig, AstrollavaTextConfig, AstroMllamaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/mnt/data/CVPR2025/task1_data/Llama-3.2-11B-Vision-Instruct\"\n",
    "vision_config = MllamaVisionConfig.from_pretrained(checkpoint)\n",
    "text_config = MllamaTextConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 8, 13, 18, 19, 24, 28, 34, 39, 40]\n",
      "[3, 8, 13, 18, 24, 28, 34, 39]\n",
      "[19]\n",
      "[40]\n"
     ]
    }
   ],
   "source": [
    "# add 2 layers, spectrum first\n",
    "original_cross_attention_layers = [3, 8, 13, 18, 23, 27, 33, 38]\n",
    "new_cross_attention_layers = []\n",
    "new_vision_cross_attention_layers = []\n",
    "new_spec_cross_attention_layers = []\n",
    "new_structure_cross_attention_layers = []\n",
    "num_new_layers = 0\n",
    "for x in original_cross_attention_layers:\n",
    "    new_vision = x + num_new_layers\n",
    "    new_vision_cross_attention_layers.append(new_vision)\n",
    "    if x == 18:\n",
    "        new_spec = x + num_new_layers + 1\n",
    "        num_new_layers += 1\n",
    "        new_spec_cross_attention_layers.append(new_spec)\n",
    "    elif x == 38:\n",
    "        new_structure = x + num_new_layers + 1\n",
    "        num_new_layers += 1\n",
    "        new_structure_cross_attention_layers.append(new_structure)   \n",
    "    \n",
    "new_cross_attention_layers = new_vision_cross_attention_layers + new_spec_cross_attention_layers + new_structure_cross_attention_layers\n",
    "new_cross_attention_layers = sorted(new_cross_attention_layers)\n",
    "print(new_cross_attention_layers)\n",
    "print(new_vision_cross_attention_layers)\n",
    "print(new_spec_cross_attention_layers)\n",
    "print(new_structure_cross_attention_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_config = AstrollavaTextConfig.from_pretrained(\"/mnt/data/CVPR2025/task1_data/Llama-3.2-11B-Vision-Instruct\",\n",
    "        cross_attention_layers=new_cross_attention_layers,\n",
    "        structure_cross_attention_layers = new_structure_cross_attention_layers,\n",
    "        spectrum_cross_attention_layers  = new_spec_cross_attention_layers,\n",
    "        vision_cross_attention_layers  = new_vision_cross_attention_layers,\n",
    "        num_hidden_layers = 40 + 2,\n",
    "        rope_scaling={\n",
    "            \"factor\": 8.0,\n",
    "            \"high_freq_factor\": 4.0,\n",
    "            \"low_freq_factor\": 1.0,\n",
    "            \"original_max_position_embeddings\": 8192,\n",
    "            \"rope_type\": \"llama3\"\n",
    "        },\n",
    "        torch_dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AstrollavaTextConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"cross_attention_layers\": [\n",
       "    3,\n",
       "    8,\n",
       "    13,\n",
       "    18,\n",
       "    19,\n",
       "    24,\n",
       "    28,\n",
       "    34,\n",
       "    39,\n",
       "    40\n",
       "  ],\n",
       "  \"dropout\": 0,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"model_type\": \"mllama_text_model\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 42,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pad_token_id\": 128004,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 8.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"spectrum_cross_attention_layers\": [\n",
       "    19\n",
       "  ],\n",
       "  \"spectrum_output_dim\": 1024,\n",
       "  \"structure_cross_attention_layers\": [\n",
       "    40\n",
       "  ],\n",
       "  \"structure_output_dim\": 1024,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vision_cross_attention_layers\": [\n",
       "    3,\n",
       "    8,\n",
       "    13,\n",
       "    18,\n",
       "    24,\n",
       "    28,\n",
       "    34,\n",
       "    39\n",
       "  ],\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = AstrollavaConfig.from_pretrained(\"/mnt/data/CVPR2025/task1_data/Llama-3.2-11B-Vision-Instruct\", text_config=new_text_config, vision_config=vision_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_config is None, using default mllama vision config\n",
      "text_config is None, using default mllama text config\n"
     ]
    }
   ],
   "source": [
    "new_config.save_pretrained(\"/mnt/data/CVPR2025/task1_data/astra-llava-add2-spec-first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = AstroMllamaForConditionalGeneration(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AstroMllamaForConditionalGeneration(\n",
       "  (language_model): AstroMllamaForCausalLM(\n",
       "    (model): AstroMllamaTextModel(\n",
       "      (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (3): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (8): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (13): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (18-19): 2 x MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (20-23): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (24): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (25-27): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (28): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (29-33): 5 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (34): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (35-38): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (39-40): 2 x MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (41): MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): MllamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (structure_modal_projector): Linear(in_features=256, out_features=4096, bias=True)\n",
       "  (spectrum_modal_projector): Linear(in_features=1024, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: language_model.model.layers.23.self_attn.q_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.23.self_attn.k_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.23.self_attn.v_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.23.self_attn.o_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.33.self_attn.q_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.33.self_attn.k_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.33.self_attn.v_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.33.self_attn.o_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.34.cross_attn_attn_gate not found in pretrained weights\n",
      "Warning: language_model.model.layers.34.cross_attn_mlp_gate not found in pretrained weights\n",
      "Warning: language_model.model.layers.34.cross_attn.q_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.34.cross_attn.k_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.34.cross_attn.v_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.34.cross_attn.o_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.34.cross_attn.q_norm.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.34.cross_attn.k_norm.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.38.self_attn.q_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.38.self_attn.k_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.38.self_attn.v_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.38.self_attn.o_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.39.cross_attn_attn_gate not found in pretrained weights\n",
      "Warning: language_model.model.layers.39.cross_attn_mlp_gate not found in pretrained weights\n",
      "Warning: language_model.model.layers.39.cross_attn.q_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.39.cross_attn.k_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.39.cross_attn.v_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.39.cross_attn.o_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.39.cross_attn.q_norm.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.39.cross_attn.k_norm.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.self_attn.q_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.self_attn.k_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.self_attn.v_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.self_attn.o_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.mlp.gate_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.mlp.up_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.mlp.down_proj.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.input_layernorm.weight not found in pretrained weights\n",
      "Warning: language_model.model.layers.41.post_attention_layernorm.weight not found in pretrained weights\n",
      "\n",
      "Weight mapping summary:\n",
      "- Spectrum adapter (layer 19) initialized from layer 18\n",
      "- Structure adapter (layer 40) initialized from layer 38\n",
      "- Vision cross attention layers mapped: [(3, 3), (8, 8), (13, 13), (18, 18), (23, 24), (27, 28), (33, 34), (38, 39)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "def load_pretrained_weights(checkpoint_dir):\n",
    "    \"\"\"加载预训练权重\"\"\"\n",
    "    with open(os.path.join(checkpoint_dir, \"model.safetensors.index.json\"), 'r') as f:\n",
    "        weight_map = json.load(f)[\"weight_map\"]\n",
    "    \n",
    "    state_dict = {}\n",
    "    for key, filename in weight_map.items():\n",
    "        weights = load_file(os.path.join(checkpoint_dir, filename))\n",
    "        if key in weights:\n",
    "            state_dict[key] = weights[key]\n",
    "    \n",
    "    return state_dict\n",
    "\n",
    "def map_weights(new_model, checkpoint_dir):\n",
    "    \"\"\"映射权重到新模型\"\"\"\n",
    "    # 加载预训练权重\n",
    "    pretrained_weights = load_pretrained_weights(checkpoint_dir)\n",
    "    \n",
    "    # 获取新模型的state dict\n",
    "    new_state_dict = new_model.state_dict()\n",
    "    \n",
    "    # 定义层的映射关系\n",
    "    original_cross_attention_layers = [3, 8, 13, 18, 23, 27, 33, 38]\n",
    "    new_vision_cross_attention_layers = [3, 8, 13, 18, 24, 28, 34, 39]\n",
    "    new_spec_cross_attention_layers = [19]  # 使用layer 18的权重初始化\n",
    "    new_structure_cross_attention_layers = [40]  # 使用layer 38的权重初始化\n",
    "    \n",
    "    # 初始化映射字典\n",
    "    mapped_state_dict = {}\n",
    "    \n",
    "    for key in new_state_dict.keys():\n",
    "        # 处理projector层\n",
    "        if 'spectrum_modal_projector' in key or 'structure_modal_projector' in key:\n",
    "            mapped_state_dict[key] = new_state_dict[key]\n",
    "            continue\n",
    "            \n",
    "        # 处理新增的spectrum cross attention层\n",
    "        if any(f'language_model.model.layers.{idx}' in key for idx in new_spec_cross_attention_layers):\n",
    "            # 使用layer 18的权重\n",
    "            source_key = key.replace(f'.19.', '.18.')\n",
    "            if source_key in pretrained_weights:\n",
    "                mapped_state_dict[key] = pretrained_weights[source_key].clone()\n",
    "            else:\n",
    "                print(f\"Warning: Source key {source_key} not found for spectrum layer\")\n",
    "                mapped_state_dict[key] = new_state_dict[key]\n",
    "            continue\n",
    "            \n",
    "        # 处理新增的structure cross attention层\n",
    "        if any(f'language_model.model.layers.{idx}' in key for idx in new_structure_cross_attention_layers):\n",
    "            # 使用layer 38的权重\n",
    "            source_key = key.replace(f'.40.', '.38.')\n",
    "            if source_key in pretrained_weights:\n",
    "                mapped_state_dict[key] = pretrained_weights[source_key].clone()\n",
    "            else:\n",
    "                print(f\"Warning: Source key {source_key} not found for structure layer\")\n",
    "                mapped_state_dict[key] = new_state_dict[key]\n",
    "            continue\n",
    "            \n",
    "        # 处理vision cross attention层的映射\n",
    "        is_cross_attn = False\n",
    "        for i, new_idx in enumerate(new_vision_cross_attention_layers):\n",
    "            if f'language_model.model.layers.{new_idx}' in key and 'cross_attn' in key:\n",
    "                orig_idx = original_cross_attention_layers[i]\n",
    "                mapped_key = key.replace(f'.{new_idx}.', f'.{orig_idx}.')\n",
    "                if mapped_key in pretrained_weights:\n",
    "                    mapped_state_dict[key] = pretrained_weights[mapped_key]\n",
    "                    is_cross_attn = True\n",
    "                break\n",
    "        \n",
    "        # 其他层直接复制\n",
    "        if not is_cross_attn:\n",
    "            if key in pretrained_weights:\n",
    "                mapped_state_dict[key] = pretrained_weights[key]\n",
    "            else:\n",
    "                print(f\"Warning: {key} not found in pretrained weights\")\n",
    "                mapped_state_dict[key] = new_state_dict[key]\n",
    "    \n",
    "    # 加载权重到新模型\n",
    "    missing_keys = new_model.load_state_dict(mapped_state_dict, strict=False)\n",
    "    \n",
    "    if missing_keys.missing_keys:\n",
    "        print(\"Missing keys:\", missing_keys.missing_keys)\n",
    "    if missing_keys.unexpected_keys:\n",
    "        print(\"Unexpected keys:\", missing_keys.unexpected_keys)\n",
    "    \n",
    "    print(\"\\nWeight mapping summary:\")\n",
    "    print(\"- Spectrum adapter (layer 19) initialized from layer 18\")\n",
    "    print(\"- Structure adapter (layer 40) initialized from layer 38\")\n",
    "    print(\"- Vision cross attention layers mapped:\", list(zip(original_cross_attention_layers, new_vision_cross_attention_layers)))\n",
    "        \n",
    "    return new_model\n",
    "\n",
    "# 使用示例\n",
    "checkpoint_dir = \"/mnt/data/CVPR2025/task1_data/Llama-3.2-11B-Vision-Instruct\"\n",
    "new_model = map_weights(new_model, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_config is None, using default mllama vision config\n",
      "text_config is None, using default mllama text config\n",
      "vision_config is None, using default mllama vision config\n",
      "text_config is None, using default mllama text config\n"
     ]
    }
   ],
   "source": [
    "new_model.save_pretrained(\"/mnt/data/CVPR2025/task1_data/astra-llava-add2-spec-first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AstroMllamaForConditionalGeneration(\n",
       "  (language_model): AstroMllamaForCausalLM(\n",
       "    (model): AstroMllamaTextModel(\n",
       "      (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (3): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (8): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (13): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (18-19): 2 x MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (20-23): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (24): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (25-27): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (28): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (29-33): 5 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (34): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (35-38): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (39-40): 2 x MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (41): MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): MllamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       "  (structure_modal_projector): Linear(in_features=256, out_features=4096, bias=True)\n",
       "  (spectrum_modal_projector): Linear(in_features=1024, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
